{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第3章　透過簡單的範例學習TensorFlow的基本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在Ubuntu環境下可利用下列的命令載入IPAexGothic\n",
    "!apt-get install  -y fonts-ipaexfont\n",
    "# 刪除字型的快取\n",
    "!rm ~/.cache/matplotlib/fontList.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**若要使用字型，必須先載入IPAexGothic再執行Kernel > Restart命令。若在Windows環境底下，請使用P.076的方法安裝IPAexGothic。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**以TensorFlow執行的1+1演算**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miyakoshi\\Anaconda3\\envs\\KS\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "a = tf.constant(1, name='a')\n",
    "b = tf.constant(1, name='b')\n",
    "c = a + b\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.2取得演算結果的類型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "a = tf.constant(1, name='a')\n",
    "b = tf.constant(1, name='b')\n",
    "c = a + b\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 避免列表3.3的結果不易瀏覽而重設圖表\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.3  顯示資料流程圖的定義。由輸出結果可以發現a或add都是節點**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node {\n",
      "  name: \"a\"\n",
      "  op: \"Const\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_INT32\n",
      "        tensor_shape {\n",
      "        }\n",
      "        int_val: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"b\"\n",
      "  op: \"Const\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_INT32\n",
      "        tensor_shape {\n",
      "        }\n",
      "        int_val: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"add\"\n",
      "  op: \"Add\"\n",
      "  input: \"a\"\n",
      "  input: \"b\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "versions {\n",
      "  producer: 24\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "a = tf.constant(1, name='a')\n",
    "b = tf.constant(1, name='b')\n",
    "c = a + b\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "print(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.4  變數範例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-995cdf1ed996>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "a = tf.Variable(1, name='a')\n",
    "b = tf.constant(1, name='b')\n",
    "c = tf.assign(a, a + b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('第一次: [c, a] =', sess.run([c, a]))\n",
    "    # 變數 c更新了\n",
    "    print('第二次: [c, a] =', sess.run([c, a])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**佔位符的範例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = 2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "a = tf.placeholder(dtype=tf.int32, name='a')\n",
    "b = tf.constant(1, name='b')\n",
    "c = a + b\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('a + b =', sess.run(c, feed_dict={a: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.6  運算範例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b =  5\n",
      "a * b =  6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "a = tf.constant(2, name='a')\n",
    "b = tf.constant(3, name='b')\n",
    "c = tf.add(a, b)  # 與a + b 等值\n",
    "d = tf.multiply(a, b)  # 與a*b 等值\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('a + b = ', sess.run(c))\n",
    "    print('a * b = ', sess.run(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**向量運算的範例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b =  [5 7 9]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "a = tf.constant([1, 2, 3], name='a')\n",
    "b = tf.constant([4, 5, 6], name='b')\n",
    "c = a + b\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('a + b = ', sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**矩陣運算的範例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of a:  (2, 2)\n",
      "shape of b:  (2, 1)\n",
      "shape of c:  (2, 1)\n",
      "a = \n",
      " [[1 2]\n",
      " [3 4]]\n",
      "b = \n",
      " [[1]\n",
      " [2]]\n",
      "c = \n",
      " [[ 5]\n",
      " [11]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "a = tf.constant([[1, 2], [3, 4]], name='a')\n",
    "b = tf.constant([[1], [2]], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print('shape of a: ', a.shape)\n",
    "print('shape of b: ', b.shape)\n",
    "print('shape of c: ', c.shape)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('a = \\n', sess.run(a))\n",
    "    print('b = \\n', sess.run(b))\n",
    "    print('c = \\n', sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.9  張量的佔位符與未知的維度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a2087a0e816a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.placeholder(shape=(None, 2), dtype=tf.int32, name='a')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('-- 代入[[1, 2]] --')\n",
    "    print('a = ', sess.run(a, feed_dict={a: [[1, 2]]}))\n",
    "    print('\\n-- 代入[[1, 2], [3, 4]] --')\n",
    "    print('a = ', sess.run(a, feed_dict={a: [[1, 2], [3, 4]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.10  Session改變後，變數就被初始化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一回目 b =  2\n",
      "二回目 b =  3\n",
      "-- 新しいセッション --\n",
      "一回目 b =  2\n",
      "二回目 b =  3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "a = tf.Variable(1, name='a')\n",
    "b = tf.assign(a, a + 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('第一次 b = ', sess.run(b))\n",
    "    print('第二次 b = ', sess.run(b))\n",
    "\n",
    "# Session改變後，變數的值就還原了\n",
    "with tf.Session() as sess:\n",
    "    print('-- 新Session --')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('第一次 b = ', sess.run(b))\n",
    "    print('第二次 b = ', sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**利用Saver儲存變數**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "a = tf.Variable(1, name='a')\n",
    "b = tf.assign(a, a + 1)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(b))\n",
    "    print(sess.run(b))\n",
    "    # 將變數值儲存為 model/model.ckpt\n",
    "    saver.save(sess, 'model/model.ckpt')\n",
    "\n",
    "# 使用Saver\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 從model/model.ckpt還原變數值\n",
    "    saver.restore(sess, save_path='model/model.ckpt')\n",
    "    print(sess.run(b))\n",
    "    print(sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.12  輸出概要**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "LOG_DIR = './logs'\n",
    "\n",
    "a = tf.constant(1, name='a')\n",
    "b = tf.constant(1, name='b')\n",
    "c = a + b\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "with tf.summary.FileWriter(LOG_DIR) as writer:\n",
    "    writer.add_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認LOG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!ls ./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.13  以梯度下降法計算二次函數的最小值**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  0.98847073\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# 將參數定義為變數\n",
    "x = tf.Variable(0., name='x')\n",
    "# 利用參數定義要計算最小值的函數\n",
    "func = (x - 1)**2\n",
    "\n",
    "# learning_rate 可決定參數的調整幅度\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=0.1\n",
    ") \n",
    "# train_step 代表逐步調整 x 的操作\n",
    "train_step = optimizer.minimize(func)\n",
    "\n",
    "# 重覆執行train_step\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(20):\n",
    "        sess.run(train_step)\n",
    "    print('x = ', sess.run(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.14  下載Boston house-price資料集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow r1.5.0會因為Keras的問題而出現錯誤\n",
    "# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述的資料夾可安裝另一個keras下載。書中介紹的是以pip install keras安裝的方法，下列的命令也可以安裝。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\miyakoshi\\anaconda3\\envs\\ks\\lib\\site-packages\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\miyakoshi\\anaconda3\\envs\\ks\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\miyakoshi\\anaconda3\\envs\\ks\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\miyakoshi\\anaconda3\\envs\\ks\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\miyakoshi\\anaconda3\\envs\\ks\\lib\\site-packages (from keras)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.15  載入Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.16  將資料畫成直方圖**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# 以inline的方式顯示matplotlib的圖表\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.rcParams['font.family'] = ['IPAexGothic']\n",
    "plt.rcParams['font.size'] = 10*3\n",
    "plt.rcParams['figure.figsize'] = [18, 12]\n",
    "\n",
    "plt.hist(y_train, bins=20)\n",
    "plt.xlabel('住宅價格($1,000單位)')\n",
    "plt.ylabel('資料數')\n",
    "plt.show()\n",
    "plt.plot(x_train[:, 5], y_train, 'o')\n",
    "plt.xlabel('房間數')\n",
    "plt.ylabel('住宅價格($1,000單位)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.17  資料的標準化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c5c30e6cd8d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx_train_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_train_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_train_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "x_train_mean = x_train.mean(axis=0)\n",
    "x_train_std = x_train.std(axis=0)\n",
    "y_train_mean = y_train.mean()\n",
    "y_train_std = y_train.std()\n",
    "\n",
    "x_train = (x_train - x_train_mean)/x_train_std\n",
    "y_train = (y_train - y_train_mean)/y_train_std\n",
    "# 即使是針對x_test也使用 x_train_mean 與 x_train_std\n",
    "x_test = (x_test - x_train_mean)/x_train_std\n",
    "# 即使是針對y_test也使用 y_train_mean與 y_train_std\n",
    "y_test = (y_test - y_train_mean)/y_train_std\n",
    "\n",
    "\n",
    "plt.plot(x_train[:, 5], y_train, 'o')\n",
    "plt.xlabel('房間數(標準化之後)')\n",
    "plt.ylabel('住宅價格(標準化之後)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.18  推測住宅價格的模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解釋變數使用的佔位符\n",
    "x = tf.placeholder(tf.float32, (None, 13), name='x')\n",
    "# 正確解答資料(住宅價格)使用的佔位符\n",
    "y = tf.placeholder(tf.float32, (None, 1), name='y')\n",
    "\n",
    "# 以權重w加總解釋變數的簡單模型\n",
    "w = tf.Variable(tf.random_normal((13, 1)))\n",
    "pred = tf.matmul(x, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.19  誤差的定義與train_step的定義**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將實測值與推測值的差距平方平均視為誤差\n",
    "loss = tf.reduce_mean((y - pred)**2)\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=0.1\n",
    ")\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.20  學習的迴圈**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train_loss: 13.650951385498047\n",
      "step: 1, train_loss: 3.3175270557403564\n",
      "step: 2, train_loss: 2.1055731773376465\n",
      "step: 3, train_loss: 1.6368244886398315\n",
      "step: 4, train_loss: 1.3534704446792603\n",
      "step: 5, train_loss: 1.167392611503601\n",
      "step: 6, train_loss: 1.0399926900863647\n",
      "step: 7, train_loss: 0.9491638541221619\n",
      "step: 8, train_loss: 0.881722629070282\n",
      "step: 9, train_loss: 0.8296720385551453\n",
      "step: 10, train_loss: 0.7880745530128479\n",
      "step: 11, train_loss: 0.7538163065910339\n",
      "step: 12, train_loss: 0.7248855233192444\n",
      "step: 13, train_loss: 0.6999463438987732\n",
      "step: 14, train_loss: 0.6780856251716614\n",
      "step: 15, train_loss: 0.6586610674858093\n",
      "step: 16, train_loss: 0.6412070989608765\n",
      "step: 17, train_loss: 0.6253775954246521\n",
      "step: 18, train_loss: 0.6109085083007812\n",
      "step: 19, train_loss: 0.5975942611694336\n",
      "step: 20, train_loss: 0.5852722525596619\n",
      "step: 21, train_loss: 0.5738107562065125\n",
      "step: 22, train_loss: 0.5631024241447449\n",
      "step: 23, train_loss: 0.5530588030815125\n",
      "step: 24, train_loss: 0.543605387210846\n",
      "step: 25, train_loss: 0.5346797704696655\n",
      "step: 26, train_loss: 0.5262289047241211\n",
      "step: 27, train_loss: 0.5182070732116699\n",
      "step: 28, train_loss: 0.5105752348899841\n",
      "step: 29, train_loss: 0.5032989978790283\n",
      "step: 30, train_loss: 0.4963487982749939\n",
      "step: 31, train_loss: 0.48969852924346924\n",
      "step: 32, train_loss: 0.4833250045776367\n",
      "step: 33, train_loss: 0.47720804810523987\n",
      "step: 34, train_loss: 0.47132933139801025\n",
      "step: 35, train_loss: 0.4656725823879242\n",
      "step: 36, train_loss: 0.4602234363555908\n",
      "step: 37, train_loss: 0.45496866106987\n",
      "step: 38, train_loss: 0.44989660382270813\n",
      "step: 39, train_loss: 0.44499653577804565\n",
      "step: 40, train_loss: 0.4402586817741394\n",
      "step: 41, train_loss: 0.4356742203235626\n",
      "step: 42, train_loss: 0.4312351644039154\n",
      "step: 43, train_loss: 0.42693397402763367\n",
      "step: 44, train_loss: 0.4227640628814697\n",
      "step: 45, train_loss: 0.418719083070755\n",
      "step: 46, train_loss: 0.41479334235191345\n",
      "step: 47, train_loss: 0.4109814465045929\n",
      "step: 48, train_loss: 0.40727853775024414\n",
      "step: 49, train_loss: 0.4036799967288971\n",
      "step: 50, train_loss: 0.40018168091773987\n",
      "step: 51, train_loss: 0.39677950739860535\n",
      "step: 52, train_loss: 0.39346978068351746\n",
      "step: 53, train_loss: 0.3902491629123688\n",
      "step: 54, train_loss: 0.38711419701576233\n",
      "step: 55, train_loss: 0.38406211137771606\n",
      "step: 56, train_loss: 0.3810896873474121\n",
      "step: 57, train_loss: 0.37819451093673706\n",
      "step: 58, train_loss: 0.3753739297389984\n",
      "step: 59, train_loss: 0.3726254105567932\n",
      "step: 60, train_loss: 0.36994674801826477\n",
      "step: 61, train_loss: 0.36733561754226685\n",
      "step: 62, train_loss: 0.3647902011871338\n",
      "step: 63, train_loss: 0.3623083829879761\n",
      "step: 64, train_loss: 0.3598881661891937\n",
      "step: 65, train_loss: 0.35752782225608826\n",
      "step: 66, train_loss: 0.35522565245628357\n",
      "step: 67, train_loss: 0.3529801070690155\n",
      "step: 68, train_loss: 0.350789338350296\n",
      "step: 69, train_loss: 0.34865203499794006\n",
      "step: 70, train_loss: 0.34656664729118347\n",
      "step: 71, train_loss: 0.3445316255092621\n",
      "step: 72, train_loss: 0.3425459861755371\n",
      "step: 73, train_loss: 0.3406081199645996\n",
      "step: 74, train_loss: 0.3387168049812317\n",
      "step: 75, train_loss: 0.33687084913253784\n",
      "step: 76, train_loss: 0.33506911993026733\n",
      "step: 77, train_loss: 0.33331045508384705\n",
      "step: 78, train_loss: 0.33159366250038147\n",
      "step: 79, train_loss: 0.3299178183078766\n",
      "step: 80, train_loss: 0.32828181982040405\n",
      "step: 81, train_loss: 0.3266846537590027\n",
      "step: 82, train_loss: 0.32512539625167847\n",
      "step: 83, train_loss: 0.3236030340194702\n",
      "step: 84, train_loss: 0.3221166729927063\n",
      "step: 85, train_loss: 0.32066547870635986\n",
      "step: 86, train_loss: 0.3192485272884369\n",
      "step: 87, train_loss: 0.3178650438785553\n",
      "step: 88, train_loss: 0.31651413440704346\n",
      "step: 89, train_loss: 0.3151950538158417\n",
      "step: 90, train_loss: 0.3139069676399231\n",
      "step: 91, train_loss: 0.3126492202281952\n",
      "step: 92, train_loss: 0.3114210367202759\n",
      "step: 93, train_loss: 0.31022167205810547\n",
      "step: 94, train_loss: 0.30905041098594666\n",
      "step: 95, train_loss: 0.30790671706199646\n",
      "step: 96, train_loss: 0.3067898750305176\n",
      "step: 97, train_loss: 0.3056991696357727\n",
      "step: 98, train_loss: 0.3046340048313141\n",
      "step: 99, train_loss: 0.30359381437301636\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(100):\n",
    "        # 由於train_step會傳回None所以利用 _ 接收\n",
    "        train_loss, _ = sess.run(\n",
    "            [loss, train_step],\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                # 為了統一y_train與y的維度而使用reshape\n",
    "                y: y_train.reshape((-1, 1))\n",
    "            }\n",
    "        )\n",
    "        print('step: {}, train_loss: {}'.format(\n",
    "            step, train_loss\n",
    "        ))\n",
    "\n",
    "    # 完成學習後，利用評估資料進行預測\n",
    "    pred_ = sess.run(\n",
    "        pred,\n",
    "        feed_dict={\n",
    "            x: x_test\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**圖3.12  評估資料的實測值與預測值**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'住宅価格(標準化後)')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test[:, 5], pred_, 'o', label='預測值')\n",
    "plt.plot(x_test[:, 5], y_test, 'x', label='實際資料')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('房間數(標準化之後)')\n",
    "plt.ylabel('住宅價格(標準化之後)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.21  傳回每個小批次的產生器**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_batches(x, y, batch_size):\n",
    "    n_data = len(x)\n",
    "    indices = np.arange(n_data)\n",
    "    np.random.shuffle(indices)\n",
    "    x_shuffled = x[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    \n",
    "    # 從原始資料隨機篩選 batch_size 筆資料\n",
    "    for i in range(0, n_data, batch_size):\n",
    "        x_batch = x_shuffled[i: i + batch_size]\n",
    "        y_batch = y_shuffled[i: i + batch_size]\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表3.22  小批次學習**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train_loss: 9.742149353027344\n",
      "step: 1, train_loss: 7.045164585113525\n",
      "step: 2, train_loss: 4.47187614440918\n",
      "step: 3, train_loss: 4.923813819885254\n",
      "step: 4, train_loss: 6.007293701171875\n",
      "step: 5, train_loss: 2.4842121601104736\n",
      "step: 6, train_loss: 3.854689359664917\n",
      "step: 7, train_loss: 0.8639840483665466\n",
      "step: 8, train_loss: 1.7174854278564453\n",
      "step: 9, train_loss: 1.4592454433441162\n",
      "step: 10, train_loss: 1.9647657871246338\n",
      "step: 11, train_loss: 1.326046347618103\n",
      "step: 12, train_loss: 2.14467191696167\n",
      "step: 13, train_loss: 0.938737690448761\n",
      "step: 14, train_loss: 0.9709228277206421\n",
      "step: 15, train_loss: 0.529624342918396\n",
      "step: 16, train_loss: 1.89034104347229\n",
      "step: 17, train_loss: 0.8615410327911377\n",
      "step: 18, train_loss: 0.8829084038734436\n",
      "step: 19, train_loss: 0.7687327861785889\n",
      "step: 20, train_loss: 0.49167945981025696\n",
      "step: 21, train_loss: 0.7754210233688354\n",
      "step: 22, train_loss: 0.6223594546318054\n",
      "step: 23, train_loss: 0.5275121331214905\n",
      "step: 24, train_loss: 0.37759989500045776\n",
      "step: 25, train_loss: 0.6907036304473877\n",
      "step: 26, train_loss: 0.9413192272186279\n",
      "step: 27, train_loss: 0.7744095325469971\n",
      "step: 28, train_loss: 0.357675164937973\n",
      "step: 29, train_loss: 0.4559670686721802\n",
      "step: 30, train_loss: 0.400451123714447\n",
      "step: 31, train_loss: 0.5681686401367188\n",
      "step: 32, train_loss: 0.5209468603134155\n",
      "step: 33, train_loss: 0.7053678035736084\n",
      "step: 34, train_loss: 0.2114579826593399\n",
      "step: 35, train_loss: 0.3312654197216034\n",
      "step: 36, train_loss: 0.22592444717884064\n",
      "step: 37, train_loss: 0.2793693542480469\n",
      "step: 38, train_loss: 0.6364570260047913\n",
      "step: 39, train_loss: 0.8459186553955078\n",
      "step: 40, train_loss: 0.22962358593940735\n",
      "step: 41, train_loss: 0.609994649887085\n",
      "step: 42, train_loss: 0.37145769596099854\n",
      "step: 43, train_loss: 0.21813392639160156\n",
      "step: 44, train_loss: 0.24989664554595947\n",
      "step: 45, train_loss: 0.6182056665420532\n",
      "step: 46, train_loss: 0.5284161567687988\n",
      "step: 47, train_loss: 0.39574578404426575\n",
      "step: 48, train_loss: 0.44841575622558594\n",
      "step: 49, train_loss: 0.5325446128845215\n",
      "step: 50, train_loss: 0.34668436646461487\n",
      "step: 51, train_loss: 0.2138534039258957\n",
      "step: 52, train_loss: 0.44290870428085327\n",
      "step: 53, train_loss: 0.20936711132526398\n",
      "step: 54, train_loss: 0.36266618967056274\n",
      "step: 55, train_loss: 0.45500922203063965\n",
      "step: 56, train_loss: 0.20141272246837616\n",
      "step: 57, train_loss: 0.45460253953933716\n",
      "step: 58, train_loss: 0.35917893052101135\n",
      "step: 59, train_loss: 0.45175936818122864\n",
      "step: 60, train_loss: 0.3491211533546448\n",
      "step: 61, train_loss: 0.2661198079586029\n",
      "step: 62, train_loss: 0.3935618996620178\n",
      "step: 63, train_loss: 0.224017933011055\n",
      "step: 64, train_loss: 0.2249990999698639\n",
      "step: 65, train_loss: 0.41433286666870117\n",
      "step: 66, train_loss: 0.30061978101730347\n",
      "step: 67, train_loss: 0.36527079343795776\n",
      "step: 68, train_loss: 0.2109624594449997\n",
      "step: 69, train_loss: 0.297299861907959\n",
      "step: 70, train_loss: 0.20073547959327698\n",
      "step: 71, train_loss: 0.18335966765880585\n",
      "step: 72, train_loss: 0.5098886489868164\n",
      "step: 73, train_loss: 0.4625084102153778\n",
      "step: 74, train_loss: 0.3192313611507416\n",
      "step: 75, train_loss: 0.44950205087661743\n",
      "step: 76, train_loss: 0.1836060881614685\n",
      "step: 77, train_loss: 0.1214098185300827\n",
      "step: 78, train_loss: 0.3226090669631958\n",
      "step: 79, train_loss: 0.2703654170036316\n",
      "step: 80, train_loss: 0.2525762617588043\n",
      "step: 81, train_loss: 0.35269051790237427\n",
      "step: 82, train_loss: 0.1492627114057541\n",
      "step: 83, train_loss: 0.1210491955280304\n",
      "step: 84, train_loss: 0.22183622419834137\n",
      "step: 85, train_loss: 0.4816361665725708\n",
      "step: 86, train_loss: 0.3633299171924591\n",
      "step: 87, train_loss: 0.25012141466140747\n",
      "step: 88, train_loss: 0.3069716691970825\n",
      "step: 89, train_loss: 0.25095611810684204\n",
      "step: 90, train_loss: 0.8894525766372681\n",
      "step: 91, train_loss: 0.6391226053237915\n",
      "step: 92, train_loss: 0.23657770454883575\n",
      "step: 93, train_loss: 0.18733108043670654\n",
      "step: 94, train_loss: 0.2681289613246918\n",
      "step: 95, train_loss: 0.3155003786087036\n",
      "step: 96, train_loss: 0.38805854320526123\n",
      "step: 97, train_loss: 0.24493834376335144\n",
      "step: 98, train_loss: 0.2581242322921753\n",
      "step: 99, train_loss: 0.3803553283214569\n",
      "step: 100, train_loss: 0.2771930992603302\n",
      "step: 101, train_loss: 0.2024356871843338\n",
      "step: 102, train_loss: 0.5224393010139465\n",
      "step: 103, train_loss: 0.32245057821273804\n",
      "step: 104, train_loss: 0.20226356387138367\n",
      "step: 105, train_loss: 0.2356109917163849\n",
      "step: 106, train_loss: 0.5942688584327698\n",
      "step: 107, train_loss: 0.2730334997177124\n",
      "step: 108, train_loss: 0.2345827966928482\n",
      "step: 109, train_loss: 0.255505234003067\n",
      "step: 110, train_loss: 0.26113268733024597\n",
      "step: 111, train_loss: 0.2879914939403534\n",
      "step: 112, train_loss: 0.2814438045024872\n",
      "step: 113, train_loss: 0.212116077542305\n",
      "step: 114, train_loss: 0.499188631772995\n",
      "step: 115, train_loss: 0.18169298768043518\n",
      "step: 116, train_loss: 0.5341112017631531\n",
      "step: 117, train_loss: 0.24586711823940277\n",
      "step: 118, train_loss: 0.14850778877735138\n",
      "step: 119, train_loss: 0.3833875358104706\n",
      "step: 120, train_loss: 0.32544177770614624\n",
      "step: 121, train_loss: 0.2818676829338074\n",
      "step: 122, train_loss: 0.3184433579444885\n",
      "step: 123, train_loss: 0.1561334729194641\n",
      "step: 124, train_loss: 0.23463672399520874\n",
      "step: 125, train_loss: 0.33072012662887573\n",
      "step: 126, train_loss: 0.3023776113986969\n",
      "step: 127, train_loss: 0.36751788854599\n",
      "step: 128, train_loss: 0.49441462755203247\n",
      "step: 129, train_loss: 0.5724676251411438\n",
      "step: 130, train_loss: 0.30518102645874023\n",
      "step: 131, train_loss: 0.18844780325889587\n",
      "step: 132, train_loss: 0.328402042388916\n",
      "step: 133, train_loss: 0.4544360041618347\n",
      "step: 134, train_loss: 0.16577833890914917\n",
      "step: 135, train_loss: 0.1374809443950653\n",
      "step: 136, train_loss: 0.35824286937713623\n",
      "step: 137, train_loss: 0.20578286051750183\n",
      "step: 138, train_loss: 0.23972058296203613\n",
      "step: 139, train_loss: 0.31438958644866943\n",
      "step: 140, train_loss: 0.3456476926803589\n",
      "step: 141, train_loss: 0.22456026077270508\n",
      "step: 142, train_loss: 0.5593701601028442\n",
      "step: 143, train_loss: 0.25621870160102844\n",
      "step: 144, train_loss: 0.42777013778686523\n",
      "step: 145, train_loss: 0.3019375205039978\n",
      "step: 146, train_loss: 0.28300949931144714\n",
      "step: 147, train_loss: 0.2510647773742676\n",
      "step: 148, train_loss: 0.2620217204093933\n",
      "step: 149, train_loss: 0.1595969796180725\n",
      "step: 150, train_loss: 0.19966109097003937\n",
      "step: 151, train_loss: 0.46520769596099854\n",
      "step: 152, train_loss: 0.3007678687572479\n",
      "step: 153, train_loss: 0.3798980712890625\n",
      "step: 154, train_loss: 0.4351109564304352\n",
      "step: 155, train_loss: 0.22999556362628937\n",
      "step: 156, train_loss: 0.3046632707118988\n",
      "step: 157, train_loss: 0.2235364317893982\n",
      "step: 158, train_loss: 0.28233450651168823\n",
      "step: 159, train_loss: 0.4999905824661255\n",
      "step: 160, train_loss: 0.3043655753135681\n",
      "step: 161, train_loss: 0.31151488423347473\n",
      "step: 162, train_loss: 0.38820135593414307\n",
      "step: 163, train_loss: 0.30212917923927307\n",
      "step: 164, train_loss: 0.2810637354850769\n",
      "step: 165, train_loss: 0.33371737599372864\n",
      "step: 166, train_loss: 0.1737058460712433\n",
      "step: 167, train_loss: 0.2801164984703064\n",
      "step: 168, train_loss: 0.17223623394966125\n",
      "step: 169, train_loss: 0.3327016234397888\n",
      "step: 170, train_loss: 0.3833642899990082\n",
      "step: 171, train_loss: 0.25048473477363586\n",
      "step: 172, train_loss: 0.42832517623901367\n",
      "step: 173, train_loss: 0.23224875330924988\n",
      "step: 174, train_loss: 0.1289667785167694\n",
      "step: 175, train_loss: 0.24840901792049408\n",
      "step: 176, train_loss: 0.25866472721099854\n",
      "step: 177, train_loss: 0.3850868344306946\n",
      "step: 178, train_loss: 0.27211910486221313\n",
      "step: 179, train_loss: 0.255611777305603\n",
      "step: 180, train_loss: 0.17951099574565887\n",
      "step: 181, train_loss: 0.6037037968635559\n",
      "step: 182, train_loss: 0.2623433768749237\n",
      "step: 183, train_loss: 0.5202375054359436\n",
      "step: 184, train_loss: 0.15470680594444275\n",
      "step: 185, train_loss: 0.2787286043167114\n",
      "step: 186, train_loss: 0.5271725654602051\n",
      "step: 187, train_loss: 0.28260213136672974\n",
      "step: 188, train_loss: 0.4373724162578583\n",
      "step: 189, train_loss: 0.35380592942237854\n",
      "step: 190, train_loss: 0.20977889001369476\n",
      "step: 191, train_loss: 0.2953522503376007\n",
      "step: 192, train_loss: 0.16167834401130676\n",
      "step: 193, train_loss: 0.24306289851665497\n",
      "step: 194, train_loss: 0.13569916784763336\n",
      "step: 195, train_loss: 0.17680290341377258\n",
      "step: 196, train_loss: 0.36121121048927307\n",
      "step: 197, train_loss: 0.4457497000694275\n",
      "step: 198, train_loss: 0.47607189416885376\n",
      "step: 199, train_loss: 0.419829785823822\n",
      "step: 200, train_loss: 0.22253528237342834\n",
      "step: 201, train_loss: 0.14313970506191254\n",
      "step: 202, train_loss: 0.20472434163093567\n",
      "step: 203, train_loss: 0.18760794401168823\n",
      "step: 204, train_loss: 0.21962684392929077\n",
      "step: 205, train_loss: 0.2394867241382599\n",
      "step: 206, train_loss: 0.3151812255382538\n",
      "step: 207, train_loss: 0.22675541043281555\n",
      "step: 208, train_loss: 0.21439602971076965\n",
      "step: 209, train_loss: 0.1621241420507431\n",
      "step: 210, train_loss: 0.18117964267730713\n",
      "step: 211, train_loss: 0.1843603402376175\n",
      "step: 212, train_loss: 0.6344811320304871\n",
      "step: 213, train_loss: 0.4275130033493042\n",
      "step: 214, train_loss: 0.1762380599975586\n",
      "step: 215, train_loss: 0.18691731989383698\n",
      "step: 216, train_loss: 0.2420571893453598\n",
      "step: 217, train_loss: 0.25511181354522705\n",
      "step: 218, train_loss: 0.2385556995868683\n",
      "step: 219, train_loss: 0.20214611291885376\n",
      "step: 220, train_loss: 0.42773208022117615\n",
      "step: 221, train_loss: 0.2915445566177368\n",
      "step: 222, train_loss: 0.2692391872406006\n",
      "step: 223, train_loss: 0.2245393991470337\n",
      "step: 224, train_loss: 0.36822524666786194\n",
      "step: 225, train_loss: 0.225798100233078\n",
      "step: 226, train_loss: 0.2424313724040985\n",
      "step: 227, train_loss: 0.561037003993988\n",
      "step: 228, train_loss: 0.16211800277233124\n",
      "step: 229, train_loss: 0.24480101466178894\n",
      "step: 230, train_loss: 0.36976733803749084\n",
      "step: 231, train_loss: 0.39901530742645264\n",
      "step: 232, train_loss: 0.34998011589050293\n",
      "step: 233, train_loss: 0.37668490409851074\n",
      "step: 234, train_loss: 0.25654399394989014\n",
      "step: 235, train_loss: 0.20618395507335663\n",
      "step: 236, train_loss: 0.18995127081871033\n",
      "step: 237, train_loss: 0.1720568686723709\n",
      "step: 238, train_loss: 0.5151106715202332\n",
      "step: 239, train_loss: 0.2981260418891907\n",
      "step: 240, train_loss: 0.5035083889961243\n",
      "step: 241, train_loss: 0.22434936463832855\n",
      "step: 242, train_loss: 0.3987136483192444\n",
      "step: 243, train_loss: 0.2675967812538147\n",
      "step: 244, train_loss: 0.09956113994121552\n",
      "step: 245, train_loss: 0.2419603019952774\n",
      "step: 246, train_loss: 0.1711384803056717\n",
      "step: 247, train_loss: 0.1612623631954193\n",
      "step: 248, train_loss: 0.23494283854961395\n",
      "step: 249, train_loss: 0.2480393350124359\n",
      "step: 250, train_loss: 0.3953339457511902\n",
      "step: 251, train_loss: 0.24370811879634857\n",
      "step: 252, train_loss: 0.4587537944316864\n",
      "step: 253, train_loss: 0.2601570785045624\n",
      "step: 254, train_loss: 0.16101813316345215\n",
      "step: 255, train_loss: 0.3118688464164734\n",
      "step: 256, train_loss: 0.32292985916137695\n",
      "step: 257, train_loss: 0.3781607747077942\n",
      "step: 258, train_loss: 0.2681783437728882\n",
      "step: 259, train_loss: 0.33564645051956177\n",
      "step: 260, train_loss: 0.3876127004623413\n",
      "step: 261, train_loss: 0.24313779175281525\n",
      "step: 262, train_loss: 0.1433352828025818\n",
      "step: 263, train_loss: 0.33339935541152954\n",
      "step: 264, train_loss: 0.27396732568740845\n",
      "step: 265, train_loss: 0.18460537493228912\n",
      "step: 266, train_loss: 0.5035808086395264\n",
      "step: 267, train_loss: 0.1895965039730072\n",
      "step: 268, train_loss: 0.13927258551120758\n",
      "step: 269, train_loss: 0.5512468218803406\n",
      "step: 270, train_loss: 0.296099454164505\n",
      "step: 271, train_loss: 0.15203580260276794\n",
      "step: 272, train_loss: 0.37385982275009155\n",
      "step: 273, train_loss: 0.11874692887067795\n",
      "step: 274, train_loss: 0.3408139944076538\n",
      "step: 275, train_loss: 0.44330599904060364\n",
      "step: 276, train_loss: 0.3283645808696747\n",
      "step: 277, train_loss: 0.3842025399208069\n",
      "step: 278, train_loss: 0.22782696783542633\n",
      "step: 279, train_loss: 0.20205062627792358\n",
      "step: 280, train_loss: 0.3053702712059021\n",
      "step: 281, train_loss: 0.39109086990356445\n",
      "step: 282, train_loss: 0.4428676962852478\n",
      "step: 283, train_loss: 0.41534173488616943\n",
      "step: 284, train_loss: 0.26169541478157043\n",
      "step: 285, train_loss: 0.318380206823349\n",
      "step: 286, train_loss: 0.2137378752231598\n",
      "step: 287, train_loss: 0.7149268388748169\n",
      "step: 288, train_loss: 0.33407843112945557\n",
      "step: 289, train_loss: 0.4122207760810852\n",
      "step: 290, train_loss: 0.3544411063194275\n",
      "step: 291, train_loss: 0.1660410612821579\n",
      "step: 292, train_loss: 0.2515160143375397\n",
      "step: 293, train_loss: 0.19653749465942383\n",
      "step: 294, train_loss: 0.28200221061706543\n",
      "step: 295, train_loss: 0.3073424696922302\n",
      "step: 296, train_loss: 0.33153074979782104\n",
      "step: 297, train_loss: 0.23784390091896057\n",
      "step: 298, train_loss: 0.15025848150253296\n",
      "step: 299, train_loss: 0.18899260461330414\n",
      "step: 300, train_loss: 0.3721845746040344\n",
      "step: 301, train_loss: 0.5441346168518066\n",
      "step: 302, train_loss: 0.2965337336063385\n",
      "step: 303, train_loss: 0.13957622647285461\n",
      "step: 304, train_loss: 0.29687464237213135\n",
      "step: 305, train_loss: 0.256138414144516\n",
      "step: 306, train_loss: 0.30616602301597595\n",
      "step: 307, train_loss: 0.12554483115673065\n",
      "step: 308, train_loss: 0.2130238264799118\n",
      "step: 309, train_loss: 0.3422975242137909\n",
      "step: 310, train_loss: 0.14525410532951355\n",
      "step: 311, train_loss: 0.8042227625846863\n",
      "step: 312, train_loss: 0.39161115884780884\n",
      "step: 313, train_loss: 0.12548282742500305\n",
      "step: 314, train_loss: 0.18617331981658936\n",
      "step: 315, train_loss: 0.21729183197021484\n",
      "step: 316, train_loss: 0.38512372970581055\n",
      "step: 317, train_loss: 0.5851203203201294\n",
      "step: 318, train_loss: 0.3468955159187317\n",
      "step: 319, train_loss: 0.38280215859413147\n",
      "step: 320, train_loss: 0.46788695454597473\n",
      "step: 321, train_loss: 0.2262074053287506\n",
      "step: 322, train_loss: 0.16226261854171753\n",
      "step: 323, train_loss: 0.14687474071979523\n",
      "step: 324, train_loss: 0.23830680549144745\n",
      "step: 325, train_loss: 0.23371067643165588\n",
      "step: 326, train_loss: 0.15621019899845123\n",
      "step: 327, train_loss: 0.21627338230609894\n",
      "step: 328, train_loss: 0.2634113132953644\n",
      "step: 329, train_loss: 0.15237337350845337\n",
      "step: 330, train_loss: 0.2364957332611084\n",
      "step: 331, train_loss: 0.47553932666778564\n",
      "step: 332, train_loss: 0.23714503645896912\n",
      "step: 333, train_loss: 0.41728392243385315\n",
      "step: 334, train_loss: 0.2447345107793808\n",
      "step: 335, train_loss: 0.4238108992576599\n",
      "step: 336, train_loss: 0.37821248173713684\n",
      "step: 337, train_loss: 0.16182053089141846\n",
      "step: 338, train_loss: 0.2704327404499054\n",
      "step: 339, train_loss: 0.3087482750415802\n",
      "step: 340, train_loss: 0.10602240264415741\n",
      "step: 341, train_loss: 0.2661932110786438\n",
      "step: 342, train_loss: 0.15892690420150757\n",
      "step: 343, train_loss: 0.18353572487831116\n",
      "step: 344, train_loss: 0.5631427764892578\n",
      "step: 345, train_loss: 0.3744398057460785\n",
      "step: 346, train_loss: 0.2222658097743988\n",
      "step: 347, train_loss: 0.346381276845932\n",
      "step: 348, train_loss: 0.1861100047826767\n",
      "step: 349, train_loss: 0.3058713674545288\n",
      "step: 350, train_loss: 0.35689812898635864\n",
      "step: 351, train_loss: 0.22830021381378174\n",
      "step: 352, train_loss: 0.2571847140789032\n",
      "step: 353, train_loss: 0.4391466975212097\n",
      "step: 354, train_loss: 0.21563862264156342\n",
      "step: 355, train_loss: 0.27083051204681396\n",
      "step: 356, train_loss: 0.2617769241333008\n",
      "step: 357, train_loss: 0.38552650809288025\n",
      "step: 358, train_loss: 0.21672385931015015\n",
      "step: 359, train_loss: 0.5173541903495789\n",
      "step: 360, train_loss: 0.2663135826587677\n",
      "step: 361, train_loss: 0.2731917202472687\n",
      "step: 362, train_loss: 0.21049511432647705\n",
      "step: 363, train_loss: 0.1772426813840866\n",
      "step: 364, train_loss: 0.1626054048538208\n",
      "step: 365, train_loss: 0.17413517832756042\n",
      "step: 366, train_loss: 0.17275753617286682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 367, train_loss: 0.24581018090248108\n",
      "step: 368, train_loss: 0.47108593583106995\n",
      "step: 369, train_loss: 0.3877979516983032\n",
      "step: 370, train_loss: 0.24426457285881042\n",
      "step: 371, train_loss: 0.38474783301353455\n",
      "step: 372, train_loss: 0.28496235609054565\n",
      "step: 373, train_loss: 0.34617382287979126\n",
      "step: 374, train_loss: 0.20211291313171387\n",
      "step: 375, train_loss: 0.24644865095615387\n",
      "step: 376, train_loss: 0.16090063750743866\n",
      "step: 377, train_loss: 0.24138130247592926\n",
      "step: 378, train_loss: 0.7508807182312012\n",
      "step: 379, train_loss: 0.14655545353889465\n",
      "step: 380, train_loss: 0.1842559427022934\n",
      "step: 381, train_loss: 0.36207878589630127\n",
      "step: 382, train_loss: 0.26034751534461975\n",
      "step: 383, train_loss: 0.14954619109630585\n",
      "step: 384, train_loss: 0.22529521584510803\n",
      "step: 385, train_loss: 0.16897538304328918\n",
      "step: 386, train_loss: 0.1727653443813324\n",
      "step: 387, train_loss: 0.6522061228752136\n",
      "step: 388, train_loss: 0.1384202390909195\n",
      "step: 389, train_loss: 0.42168816924095154\n",
      "step: 390, train_loss: 0.18899568915367126\n",
      "step: 391, train_loss: 0.3932354748249054\n",
      "step: 392, train_loss: 0.24565988779067993\n",
      "step: 393, train_loss: 0.2804810106754303\n",
      "step: 394, train_loss: 0.3460844159126282\n",
      "step: 395, train_loss: 0.15859442949295044\n",
      "step: 396, train_loss: 0.12661878764629364\n",
      "step: 397, train_loss: 0.14516682922840118\n",
      "step: 398, train_loss: 0.4661087393760681\n",
      "step: 399, train_loss: 0.3130975663661957\n",
      "step: 400, train_loss: 0.3282567858695984\n",
      "step: 401, train_loss: 0.5198416709899902\n",
      "step: 402, train_loss: 0.21287474036216736\n",
      "step: 403, train_loss: 0.16087208688259125\n",
      "step: 404, train_loss: 0.1343744695186615\n",
      "step: 405, train_loss: 0.3496832251548767\n",
      "step: 406, train_loss: 0.10075058788061142\n",
      "step: 407, train_loss: 0.26581719517707825\n",
      "step: 408, train_loss: 0.2745744585990906\n",
      "step: 409, train_loss: 0.6736118793487549\n",
      "step: 410, train_loss: 0.44260427355766296\n",
      "step: 411, train_loss: 0.4800640642642975\n",
      "step: 412, train_loss: 0.23863869905471802\n",
      "step: 413, train_loss: 0.2958320379257202\n",
      "step: 414, train_loss: 0.22238707542419434\n",
      "step: 415, train_loss: 0.1841944456100464\n",
      "step: 416, train_loss: 0.24031305313110352\n",
      "step: 417, train_loss: 0.43389859795570374\n",
      "step: 418, train_loss: 0.34948235750198364\n",
      "step: 419, train_loss: 0.294281005859375\n",
      "step: 420, train_loss: 0.17670860886573792\n",
      "step: 421, train_loss: 0.20782658457756042\n",
      "step: 422, train_loss: 0.46759307384490967\n",
      "step: 423, train_loss: 0.22897738218307495\n",
      "step: 424, train_loss: 0.23452523350715637\n",
      "step: 425, train_loss: 0.46981269121170044\n",
      "step: 426, train_loss: 0.18686816096305847\n",
      "step: 427, train_loss: 0.2641880512237549\n",
      "step: 428, train_loss: 0.4251156449317932\n",
      "step: 429, train_loss: 0.19767597317695618\n",
      "step: 430, train_loss: 0.34355950355529785\n",
      "step: 431, train_loss: 0.2760334610939026\n",
      "step: 432, train_loss: 0.24506132304668427\n",
      "step: 433, train_loss: 0.24329516291618347\n",
      "step: 434, train_loss: 0.21922540664672852\n",
      "step: 435, train_loss: 0.43630391359329224\n",
      "step: 436, train_loss: 0.33244526386260986\n",
      "step: 437, train_loss: 0.5672208666801453\n",
      "step: 438, train_loss: 0.2027546614408493\n",
      "step: 439, train_loss: 0.16112098097801208\n",
      "step: 440, train_loss: 0.4887591302394867\n",
      "step: 441, train_loss: 0.334068238735199\n",
      "step: 442, train_loss: 0.37701988220214844\n",
      "step: 443, train_loss: 0.2308582216501236\n",
      "step: 444, train_loss: 0.1462228000164032\n",
      "step: 445, train_loss: 0.3394657075405121\n",
      "step: 446, train_loss: 0.17263010144233704\n",
      "step: 447, train_loss: 0.24240648746490479\n",
      "step: 448, train_loss: 0.2813509702682495\n",
      "step: 449, train_loss: 0.22363929450511932\n",
      "step: 450, train_loss: 0.14341199398040771\n",
      "step: 451, train_loss: 0.22511780261993408\n",
      "step: 452, train_loss: 0.3286299407482147\n",
      "step: 453, train_loss: 0.3220064342021942\n",
      "step: 454, train_loss: 0.9422365427017212\n",
      "step: 455, train_loss: 0.40936577320098877\n",
      "step: 456, train_loss: 0.16651186347007751\n",
      "step: 457, train_loss: 0.25179794430732727\n",
      "step: 458, train_loss: 0.1803693175315857\n",
      "step: 459, train_loss: 0.2208295315504074\n",
      "step: 460, train_loss: 0.2511954605579376\n",
      "step: 461, train_loss: 0.24825066328048706\n",
      "step: 462, train_loss: 0.19360125064849854\n",
      "step: 463, train_loss: 0.23016713559627533\n",
      "step: 464, train_loss: 0.47217661142349243\n",
      "step: 465, train_loss: 0.2460947185754776\n",
      "step: 466, train_loss: 0.501380443572998\n",
      "step: 467, train_loss: 0.38919171690940857\n",
      "step: 468, train_loss: 0.4498737156391144\n",
      "step: 469, train_loss: 0.249042809009552\n",
      "step: 470, train_loss: 0.3253990709781647\n",
      "step: 471, train_loss: 0.2935295104980469\n",
      "step: 472, train_loss: 0.1617199331521988\n",
      "step: 473, train_loss: 0.3860169053077698\n",
      "step: 474, train_loss: 0.14804843068122864\n",
      "step: 475, train_loss: 0.12669815123081207\n",
      "step: 476, train_loss: 0.4195723235607147\n",
      "step: 477, train_loss: 0.5570024251937866\n",
      "step: 478, train_loss: 0.14554190635681152\n",
      "step: 479, train_loss: 0.267039954662323\n",
      "step: 480, train_loss: 0.27641597390174866\n",
      "step: 481, train_loss: 0.5770308971405029\n",
      "step: 482, train_loss: 0.19970491528511047\n",
      "step: 483, train_loss: 0.34794026613235474\n",
      "step: 484, train_loss: 0.2236720621585846\n",
      "step: 485, train_loss: 0.2423085719347\n",
      "step: 486, train_loss: 0.5458474159240723\n",
      "step: 487, train_loss: 0.304210364818573\n",
      "step: 488, train_loss: 0.25665032863616943\n",
      "step: 489, train_loss: 0.1674862504005432\n",
      "step: 490, train_loss: 0.2669113278388977\n",
      "step: 491, train_loss: 0.33183687925338745\n",
      "step: 492, train_loss: 0.11602844297885895\n",
      "step: 493, train_loss: 0.19183075428009033\n",
      "step: 494, train_loss: 0.17786473035812378\n",
      "step: 495, train_loss: 0.12270478159189224\n",
      "step: 496, train_loss: 0.4284483790397644\n",
      "step: 497, train_loss: 0.32475683093070984\n",
      "step: 498, train_loss: 0.8224460482597351\n",
      "step: 499, train_loss: 0.14364846050739288\n",
      "step: 500, train_loss: 0.21063391864299774\n",
      "step: 501, train_loss: 0.21818028390407562\n",
      "step: 502, train_loss: 0.18233829736709595\n",
      "step: 503, train_loss: 0.2573694586753845\n",
      "step: 504, train_loss: 0.12886399030685425\n",
      "step: 505, train_loss: 0.30168506503105164\n",
      "step: 506, train_loss: 0.5412696599960327\n",
      "step: 507, train_loss: 0.2241533398628235\n",
      "step: 508, train_loss: 0.14772935211658478\n",
      "step: 509, train_loss: 0.30029523372650146\n",
      "step: 510, train_loss: 0.2804434895515442\n",
      "step: 511, train_loss: 0.3347381353378296\n",
      "step: 512, train_loss: 0.27808505296707153\n",
      "step: 513, train_loss: 0.3648822009563446\n",
      "step: 514, train_loss: 0.3903481960296631\n",
      "step: 515, train_loss: 0.7758907079696655\n",
      "step: 516, train_loss: 0.2875593602657318\n",
      "step: 517, train_loss: 0.1461201310157776\n",
      "step: 518, train_loss: 0.22385096549987793\n",
      "step: 519, train_loss: 0.14499716460704803\n",
      "step: 520, train_loss: 0.18379442393779755\n",
      "step: 521, train_loss: 0.3748413920402527\n",
      "step: 522, train_loss: 0.289691299200058\n",
      "step: 523, train_loss: 0.2753218412399292\n",
      "step: 524, train_loss: 0.5761191844940186\n",
      "step: 525, train_loss: 0.3012463450431824\n",
      "step: 526, train_loss: 0.36685115098953247\n",
      "step: 527, train_loss: 0.36259907484054565\n",
      "step: 528, train_loss: 0.2171202301979065\n",
      "step: 529, train_loss: 0.21444961428642273\n",
      "step: 530, train_loss: 0.22756102681159973\n",
      "step: 531, train_loss: 0.23894990980625153\n",
      "step: 532, train_loss: 0.22991323471069336\n",
      "step: 533, train_loss: 0.25150585174560547\n",
      "step: 534, train_loss: 0.21903711557388306\n",
      "step: 535, train_loss: 0.15523402392864227\n",
      "step: 536, train_loss: 0.535327672958374\n",
      "step: 537, train_loss: 0.4027586281299591\n",
      "step: 538, train_loss: 0.4257737994194031\n",
      "step: 539, train_loss: 0.2540733516216278\n",
      "step: 540, train_loss: 0.3418404459953308\n",
      "step: 541, train_loss: 0.2614179253578186\n",
      "step: 542, train_loss: 0.27654018998146057\n",
      "step: 543, train_loss: 0.18249595165252686\n",
      "step: 544, train_loss: 0.3658188581466675\n",
      "step: 545, train_loss: 0.2477874457836151\n",
      "step: 546, train_loss: 0.40272006392478943\n",
      "step: 547, train_loss: 0.2243058979511261\n",
      "step: 548, train_loss: 0.09169581532478333\n",
      "step: 549, train_loss: 0.27888357639312744\n",
      "step: 550, train_loss: 0.5855798721313477\n",
      "step: 551, train_loss: 0.33830851316452026\n",
      "step: 552, train_loss: 0.2274864912033081\n",
      "step: 553, train_loss: 0.19582775235176086\n",
      "step: 554, train_loss: 0.3463030457496643\n",
      "step: 555, train_loss: 0.2594846189022064\n",
      "step: 556, train_loss: 0.3174556791782379\n",
      "step: 557, train_loss: 0.18480214476585388\n",
      "step: 558, train_loss: 0.15368592739105225\n",
      "step: 559, train_loss: 0.24147765338420868\n",
      "step: 560, train_loss: 0.4218994975090027\n",
      "step: 561, train_loss: 0.3135603368282318\n",
      "step: 562, train_loss: 0.2104932963848114\n",
      "step: 563, train_loss: 0.1339152753353119\n",
      "step: 564, train_loss: 0.3301057815551758\n",
      "step: 565, train_loss: 0.1693270355463028\n",
      "step: 566, train_loss: 0.3579574227333069\n",
      "step: 567, train_loss: 0.28116515278816223\n",
      "step: 568, train_loss: 0.27275553345680237\n",
      "step: 569, train_loss: 0.43780550360679626\n",
      "step: 570, train_loss: 0.263278603553772\n",
      "step: 571, train_loss: 0.3290608525276184\n",
      "step: 572, train_loss: 0.3856428563594818\n",
      "step: 573, train_loss: 0.22667694091796875\n",
      "step: 574, train_loss: 0.23821622133255005\n",
      "step: 575, train_loss: 0.26540407538414\n",
      "step: 576, train_loss: 0.44555437564849854\n",
      "step: 577, train_loss: 0.18860948085784912\n",
      "step: 578, train_loss: 0.32566002011299133\n",
      "step: 579, train_loss: 0.30340561270713806\n",
      "step: 580, train_loss: 0.18143020570278168\n",
      "step: 581, train_loss: 0.3052801787853241\n",
      "step: 582, train_loss: 0.2533422112464905\n",
      "step: 583, train_loss: 0.33146288990974426\n",
      "step: 584, train_loss: 0.22783157229423523\n",
      "step: 585, train_loss: 0.20040422677993774\n",
      "step: 586, train_loss: 0.22563353180885315\n",
      "step: 587, train_loss: 0.19515885412693024\n",
      "step: 588, train_loss: 0.31354111433029175\n",
      "step: 589, train_loss: 0.16566410660743713\n",
      "step: 590, train_loss: 0.5180696249008179\n",
      "step: 591, train_loss: 0.37383776903152466\n",
      "step: 592, train_loss: 0.445285439491272\n",
      "step: 593, train_loss: 0.4880048632621765\n",
      "step: 594, train_loss: 0.2133425772190094\n",
      "step: 595, train_loss: 0.3700549006462097\n",
      "step: 596, train_loss: 0.29115137457847595\n",
      "step: 597, train_loss: 0.13311097025871277\n",
      "step: 598, train_loss: 0.34268584847450256\n",
      "step: 599, train_loss: 0.31571367383003235\n",
      "step: 600, train_loss: 0.29405924677848816\n",
      "step: 601, train_loss: 0.20011040568351746\n",
      "step: 602, train_loss: 0.27096694707870483\n",
      "step: 603, train_loss: 0.27756035327911377\n",
      "step: 604, train_loss: 0.5009015202522278\n",
      "step: 605, train_loss: 0.272365540266037\n",
      "step: 606, train_loss: 0.2321404665708542\n",
      "step: 607, train_loss: 0.254772424697876\n",
      "step: 608, train_loss: 0.41626599431037903\n",
      "step: 609, train_loss: 0.20995382964611053\n",
      "step: 610, train_loss: 0.27826958894729614\n",
      "step: 611, train_loss: 0.2445305734872818\n",
      "step: 612, train_loss: 0.21701723337173462\n",
      "step: 613, train_loss: 0.23157280683517456\n",
      "step: 614, train_loss: 0.3882119059562683\n",
      "step: 615, train_loss: 0.26640141010284424\n",
      "step: 616, train_loss: 0.38987767696380615\n",
      "step: 617, train_loss: 0.4533419609069824\n",
      "step: 618, train_loss: 0.43571531772613525\n",
      "step: 619, train_loss: 0.2753217816352844\n",
      "step: 620, train_loss: 0.2625170946121216\n",
      "step: 621, train_loss: 0.15932047367095947\n",
      "step: 622, train_loss: 0.0938602164387703\n",
      "step: 623, train_loss: 0.3089272379875183\n",
      "step: 624, train_loss: 0.26968300342559814\n",
      "step: 625, train_loss: 0.1294986605644226\n",
      "step: 626, train_loss: 0.11387408524751663\n",
      "step: 627, train_loss: 0.1678558588027954\n",
      "step: 628, train_loss: 0.1835038959980011\n",
      "step: 629, train_loss: 0.31274914741516113\n",
      "step: 630, train_loss: 0.2547950744628906\n",
      "step: 631, train_loss: 0.5334224700927734\n",
      "step: 632, train_loss: 0.4779664874076843\n",
      "step: 633, train_loss: 0.33775728940963745\n",
      "step: 634, train_loss: 0.22755327820777893\n",
      "step: 635, train_loss: 0.7311148643493652\n",
      "step: 636, train_loss: 0.2519369125366211\n",
      "step: 637, train_loss: 0.3295234143733978\n",
      "step: 638, train_loss: 0.5051159858703613\n",
      "step: 639, train_loss: 0.3876006007194519\n",
      "step: 640, train_loss: 0.16919226944446564\n",
      "step: 641, train_loss: 0.15362140536308289\n",
      "step: 642, train_loss: 0.12575362622737885\n",
      "step: 643, train_loss: 0.18592481315135956\n",
      "step: 644, train_loss: 0.19729900360107422\n",
      "step: 645, train_loss: 0.18626092374324799\n",
      "step: 646, train_loss: 0.5567447543144226\n",
      "step: 647, train_loss: 0.4071732759475708\n",
      "step: 648, train_loss: 0.4894515573978424\n",
      "step: 649, train_loss: 0.4554084241390228\n",
      "step: 650, train_loss: 0.4716244041919708\n",
      "step: 651, train_loss: 0.2661390006542206\n",
      "step: 652, train_loss: 0.15522310137748718\n",
      "step: 653, train_loss: 0.4538343548774719\n",
      "step: 654, train_loss: 0.30818840861320496\n",
      "step: 655, train_loss: 0.22470048069953918\n",
      "step: 656, train_loss: 0.13785552978515625\n",
      "step: 657, train_loss: 0.18342983722686768\n",
      "step: 658, train_loss: 0.5994677543640137\n",
      "step: 659, train_loss: 0.17871986329555511\n",
      "step: 660, train_loss: 0.278868168592453\n",
      "step: 661, train_loss: 0.235597163438797\n",
      "step: 662, train_loss: 0.35580864548683167\n",
      "step: 663, train_loss: 0.8917348980903625\n",
      "step: 664, train_loss: 0.2660631835460663\n",
      "step: 665, train_loss: 0.19441257417201996\n",
      "step: 666, train_loss: 0.28917527198791504\n",
      "step: 667, train_loss: 0.1910942941904068\n",
      "step: 668, train_loss: 0.1404639035463333\n",
      "step: 669, train_loss: 0.22822719812393188\n",
      "step: 670, train_loss: 0.7725562453269958\n",
      "step: 671, train_loss: 0.28127482533454895\n",
      "step: 672, train_loss: 0.28303325176239014\n",
      "step: 673, train_loss: 0.23321856558322906\n",
      "step: 674, train_loss: 0.2113879919052124\n",
      "step: 675, train_loss: 0.20722636580467224\n",
      "step: 676, train_loss: 0.25909698009490967\n",
      "step: 677, train_loss: 0.31706535816192627\n",
      "step: 678, train_loss: 0.1368018090724945\n",
      "step: 679, train_loss: 0.6393306255340576\n",
      "step: 680, train_loss: 0.47818613052368164\n",
      "step: 681, train_loss: 0.17821139097213745\n",
      "step: 682, train_loss: 0.15554971992969513\n",
      "step: 683, train_loss: 0.31562355160713196\n",
      "step: 684, train_loss: 0.5727220177650452\n",
      "step: 685, train_loss: 0.17841628193855286\n",
      "step: 686, train_loss: 0.21523712575435638\n",
      "step: 687, train_loss: 0.5147174596786499\n",
      "step: 688, train_loss: 0.20900435745716095\n",
      "step: 689, train_loss: 0.2222379744052887\n",
      "step: 690, train_loss: 0.3507527709007263\n",
      "step: 691, train_loss: 0.21291013062000275\n",
      "step: 692, train_loss: 0.3253723680973053\n",
      "step: 693, train_loss: 0.2472304105758667\n",
      "step: 694, train_loss: 0.6986039876937866\n",
      "step: 695, train_loss: 0.40173983573913574\n",
      "step: 696, train_loss: 0.19937744736671448\n",
      "step: 697, train_loss: 0.10413318127393723\n",
      "step: 698, train_loss: 0.2513963282108307\n",
      "step: 699, train_loss: 0.21824724972248077\n",
      "step: 700, train_loss: 0.25890806317329407\n",
      "step: 701, train_loss: 0.513382077217102\n",
      "step: 702, train_loss: 0.2550559937953949\n",
      "step: 703, train_loss: 0.3271641731262207\n",
      "step: 704, train_loss: 0.4036182165145874\n",
      "step: 705, train_loss: 0.24067474901676178\n",
      "step: 706, train_loss: 0.27702194452285767\n",
      "step: 707, train_loss: 0.18049101531505585\n",
      "step: 708, train_loss: 0.2336605042219162\n",
      "step: 709, train_loss: 0.315969854593277\n",
      "step: 710, train_loss: 0.23061470687389374\n",
      "step: 711, train_loss: 0.1721639782190323\n",
      "step: 712, train_loss: 0.30186986923217773\n",
      "step: 713, train_loss: 0.34696948528289795\n",
      "step: 714, train_loss: 0.3123761713504791\n",
      "step: 715, train_loss: 0.18801194429397583\n",
      "step: 716, train_loss: 0.1092829555273056\n",
      "step: 717, train_loss: 0.15505588054656982\n",
      "step: 718, train_loss: 0.21605223417282104\n",
      "step: 719, train_loss: 0.3943437933921814\n",
      "step: 720, train_loss: 0.21359214186668396\n",
      "step: 721, train_loss: 0.5228867530822754\n",
      "step: 722, train_loss: 0.15037952363491058\n",
      "step: 723, train_loss: 0.24253827333450317\n",
      "step: 724, train_loss: 0.46848055720329285\n",
      "step: 725, train_loss: 0.42996448278427124\n",
      "step: 726, train_loss: 0.6321219205856323\n",
      "step: 727, train_loss: 0.31413865089416504\n",
      "step: 728, train_loss: 0.179594948887825\n",
      "step: 729, train_loss: 0.24480585753917694\n",
      "step: 730, train_loss: 0.42384073138237\n",
      "step: 731, train_loss: 0.22388790547847748\n",
      "step: 732, train_loss: 0.527092695236206\n",
      "step: 733, train_loss: 0.39053159952163696\n",
      "step: 734, train_loss: 0.34298741817474365\n",
      "step: 735, train_loss: 0.18350112438201904\n",
      "step: 736, train_loss: 0.2332865595817566\n",
      "step: 737, train_loss: 0.4150187373161316\n",
      "step: 738, train_loss: 0.18619605898857117\n",
      "step: 739, train_loss: 0.15162219107151031\n",
      "step: 740, train_loss: 0.6211601495742798\n",
      "step: 741, train_loss: 0.23642030358314514\n",
      "step: 742, train_loss: 0.52574622631073\n",
      "step: 743, train_loss: 0.22259403765201569\n",
      "step: 744, train_loss: 0.22654162347316742\n",
      "step: 745, train_loss: 0.2434195876121521\n",
      "step: 746, train_loss: 0.19365167617797852\n",
      "step: 747, train_loss: 0.2587083578109741\n",
      "step: 748, train_loss: 0.26583123207092285\n",
      "step: 749, train_loss: 0.23961617052555084\n",
      "step: 750, train_loss: 0.21455532312393188\n",
      "step: 751, train_loss: 0.20919880270957947\n",
      "step: 752, train_loss: 0.5375641584396362\n",
      "step: 753, train_loss: 0.5467053651809692\n",
      "step: 754, train_loss: 0.27595454454421997\n",
      "step: 755, train_loss: 0.2408175766468048\n",
      "step: 756, train_loss: 0.2379961460828781\n",
      "step: 757, train_loss: 0.22823703289031982\n",
      "step: 758, train_loss: 0.16399097442626953\n",
      "step: 759, train_loss: 0.2645755410194397\n",
      "step: 760, train_loss: 0.18233439326286316\n",
      "step: 761, train_loss: 0.21315152943134308\n",
      "step: 762, train_loss: 0.32924729585647583\n",
      "step: 763, train_loss: 0.20876222848892212\n",
      "step: 764, train_loss: 0.3956645131111145\n",
      "step: 765, train_loss: 0.5351903438568115\n",
      "step: 766, train_loss: 0.35195744037628174\n",
      "step: 767, train_loss: 0.313856303691864\n",
      "step: 768, train_loss: 0.17296180129051208\n",
      "step: 769, train_loss: 0.4089218080043793\n",
      "step: 770, train_loss: 0.4665234088897705\n",
      "step: 771, train_loss: 0.2331487536430359\n",
      "step: 772, train_loss: 0.1440998613834381\n",
      "step: 773, train_loss: 0.25691866874694824\n",
      "step: 774, train_loss: 0.28227120637893677\n",
      "step: 775, train_loss: 0.23030969500541687\n",
      "step: 776, train_loss: 0.36863020062446594\n",
      "step: 777, train_loss: 0.21651937067508698\n",
      "step: 778, train_loss: 0.47069358825683594\n",
      "step: 779, train_loss: 0.2885042726993561\n",
      "step: 780, train_loss: 0.6199356317520142\n",
      "step: 781, train_loss: 0.2238352745771408\n",
      "step: 782, train_loss: 0.30228057503700256\n",
      "step: 783, train_loss: 0.37252277135849\n",
      "step: 784, train_loss: 0.1989264190196991\n",
      "step: 785, train_loss: 0.17809048295021057\n",
      "step: 786, train_loss: 0.24807468056678772\n",
      "step: 787, train_loss: 0.22117429971694946\n",
      "step: 788, train_loss: 0.22601637244224548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 789, train_loss: 0.43423354625701904\n",
      "step: 790, train_loss: 0.2884212136268616\n",
      "step: 791, train_loss: 0.17582328617572784\n",
      "step: 792, train_loss: 0.1710638403892517\n",
      "step: 793, train_loss: 0.5895472764968872\n",
      "step: 794, train_loss: 0.25360405445098877\n",
      "step: 795, train_loss: 0.144330233335495\n",
      "step: 796, train_loss: 0.11362561583518982\n",
      "step: 797, train_loss: 0.20719507336616516\n",
      "step: 798, train_loss: 0.13562597334384918\n",
      "step: 799, train_loss: 0.36785703897476196\n",
      "step: 800, train_loss: 0.2362401783466339\n",
      "step: 801, train_loss: 0.5410900115966797\n",
      "step: 802, train_loss: 0.13290119171142578\n",
      "step: 803, train_loss: 0.5892395377159119\n",
      "step: 804, train_loss: 0.25085434317588806\n",
      "step: 805, train_loss: 0.19926287233829498\n",
      "step: 806, train_loss: 0.2382480800151825\n",
      "step: 807, train_loss: 0.2525915801525116\n",
      "step: 808, train_loss: 0.3323007822036743\n",
      "step: 809, train_loss: 0.18041487038135529\n",
      "step: 810, train_loss: 0.3506637513637543\n",
      "step: 811, train_loss: 0.2456110119819641\n",
      "step: 812, train_loss: 0.20816481113433838\n",
      "step: 813, train_loss: 0.15802410244941711\n",
      "step: 814, train_loss: 0.10624883323907852\n",
      "step: 815, train_loss: 0.5937591791152954\n",
      "step: 816, train_loss: 0.29740408062934875\n",
      "step: 817, train_loss: 0.4312777817249298\n",
      "step: 818, train_loss: 0.4074556231498718\n",
      "step: 819, train_loss: 0.3621559143066406\n",
      "step: 820, train_loss: 0.3394317626953125\n",
      "step: 821, train_loss: 0.21097630262374878\n",
      "step: 822, train_loss: 0.2040991485118866\n",
      "step: 823, train_loss: 0.18893462419509888\n",
      "step: 824, train_loss: 0.2196897566318512\n",
      "step: 825, train_loss: 0.4603899121284485\n",
      "step: 826, train_loss: 0.20757907629013062\n",
      "step: 827, train_loss: 0.3443065881729126\n",
      "step: 828, train_loss: 0.3606913387775421\n",
      "step: 829, train_loss: 0.28228870034217834\n",
      "step: 830, train_loss: 0.3012731373310089\n",
      "step: 831, train_loss: 0.5194228887557983\n",
      "step: 832, train_loss: 0.6296333074569702\n",
      "step: 833, train_loss: 0.17477357387542725\n",
      "step: 834, train_loss: 0.22035911679267883\n",
      "step: 835, train_loss: 0.15081650018692017\n",
      "step: 836, train_loss: 0.5396778583526611\n",
      "step: 837, train_loss: 0.21151362359523773\n",
      "step: 838, train_loss: 0.24379220604896545\n",
      "step: 839, train_loss: 0.27113670110702515\n",
      "step: 840, train_loss: 0.2204974889755249\n",
      "step: 841, train_loss: 0.532120943069458\n",
      "step: 842, train_loss: 0.339959055185318\n",
      "step: 843, train_loss: 0.25596320629119873\n",
      "step: 844, train_loss: 0.1542510837316513\n",
      "step: 845, train_loss: 0.19634047150611877\n",
      "step: 846, train_loss: 0.3094254434108734\n",
      "step: 847, train_loss: 0.27716636657714844\n",
      "step: 848, train_loss: 0.5871021151542664\n",
      "step: 849, train_loss: 0.40447378158569336\n",
      "step: 850, train_loss: 0.32195156812667847\n",
      "step: 851, train_loss: 0.15429644286632538\n",
      "step: 852, train_loss: 0.38131916522979736\n",
      "step: 853, train_loss: 0.4667488634586334\n",
      "step: 854, train_loss: 0.1927889585494995\n",
      "step: 855, train_loss: 0.24503937363624573\n",
      "step: 856, train_loss: 0.16267207264900208\n",
      "step: 857, train_loss: 0.16649657487869263\n",
      "step: 858, train_loss: 0.4319137632846832\n",
      "step: 859, train_loss: 0.20376920700073242\n",
      "step: 860, train_loss: 0.20339727401733398\n",
      "step: 861, train_loss: 0.21922366321086884\n",
      "step: 862, train_loss: 0.49245837330818176\n",
      "step: 863, train_loss: 0.36991631984710693\n",
      "step: 864, train_loss: 0.2514595091342926\n",
      "step: 865, train_loss: 0.24133937060832977\n",
      "step: 866, train_loss: 0.3754011392593384\n",
      "step: 867, train_loss: 0.20145247876644135\n",
      "step: 868, train_loss: 0.20986256003379822\n",
      "step: 869, train_loss: 0.2997283637523651\n",
      "step: 870, train_loss: 0.1504126340150833\n",
      "step: 871, train_loss: 0.5727342367172241\n",
      "step: 872, train_loss: 0.21193791925907135\n",
      "step: 873, train_loss: 0.21414946019649506\n",
      "step: 874, train_loss: 0.4505652189254761\n",
      "step: 875, train_loss: 0.3045145273208618\n",
      "step: 876, train_loss: 0.2697124183177948\n",
      "step: 877, train_loss: 0.16787657141685486\n",
      "step: 878, train_loss: 0.22648127377033234\n",
      "step: 879, train_loss: 0.31707385182380676\n",
      "step: 880, train_loss: 0.32488206028938293\n",
      "step: 881, train_loss: 0.22036787867546082\n",
      "step: 882, train_loss: 0.19802066683769226\n",
      "step: 883, train_loss: 0.4509212374687195\n",
      "step: 884, train_loss: 0.46479469537734985\n",
      "step: 885, train_loss: 0.2541973888874054\n",
      "step: 886, train_loss: 0.4431224763393402\n",
      "step: 887, train_loss: 0.19504670798778534\n",
      "step: 888, train_loss: 0.2618289589881897\n",
      "step: 889, train_loss: 0.18869110941886902\n",
      "step: 890, train_loss: 0.35495778918266296\n",
      "step: 891, train_loss: 0.21071240305900574\n",
      "step: 892, train_loss: 0.2854725122451782\n",
      "step: 893, train_loss: 0.25106918811798096\n",
      "step: 894, train_loss: 0.3409249186515808\n",
      "step: 895, train_loss: 0.3117426931858063\n",
      "step: 896, train_loss: 0.288203626871109\n",
      "step: 897, train_loss: 0.41848546266555786\n",
      "step: 898, train_loss: 0.17620450258255005\n",
      "step: 899, train_loss: 0.2004278302192688\n",
      "step: 900, train_loss: 0.10162633657455444\n",
      "step: 901, train_loss: 0.23496870696544647\n",
      "step: 902, train_loss: 0.18058541417121887\n",
      "step: 903, train_loss: 0.7475563883781433\n",
      "step: 904, train_loss: 0.3668506145477295\n",
      "step: 905, train_loss: 0.20222267508506775\n",
      "step: 906, train_loss: 0.3831486999988556\n",
      "step: 907, train_loss: 0.3208622336387634\n",
      "step: 908, train_loss: 0.2699342370033264\n",
      "step: 909, train_loss: 0.13149723410606384\n",
      "step: 910, train_loss: 0.2195722460746765\n",
      "step: 911, train_loss: 0.14279745519161224\n",
      "step: 912, train_loss: 0.555263340473175\n",
      "step: 913, train_loss: 0.39951181411743164\n",
      "step: 914, train_loss: 0.27079546451568604\n",
      "step: 915, train_loss: 0.5963649749755859\n",
      "step: 916, train_loss: 0.2586287260055542\n",
      "step: 917, train_loss: 0.1368483603000641\n",
      "step: 918, train_loss: 0.2980382442474365\n",
      "step: 919, train_loss: 0.2707597613334656\n",
      "step: 920, train_loss: 0.2538985311985016\n",
      "step: 921, train_loss: 0.13535785675048828\n",
      "step: 922, train_loss: 0.17944064736366272\n",
      "step: 923, train_loss: 0.22855475544929504\n",
      "step: 924, train_loss: 0.23061975836753845\n",
      "step: 925, train_loss: 0.1764390468597412\n",
      "step: 926, train_loss: 0.18243059515953064\n",
      "step: 927, train_loss: 0.3721020221710205\n",
      "step: 928, train_loss: 0.16238409280776978\n",
      "step: 929, train_loss: 0.28257542848587036\n",
      "step: 930, train_loss: 0.3170115053653717\n",
      "step: 931, train_loss: 0.2520841956138611\n",
      "step: 932, train_loss: 0.4658520221710205\n",
      "step: 933, train_loss: 0.49146196246147156\n",
      "step: 934, train_loss: 0.25859346985816956\n",
      "step: 935, train_loss: 0.33982738852500916\n",
      "step: 936, train_loss: 0.324293315410614\n",
      "step: 937, train_loss: 0.1371767818927765\n",
      "step: 938, train_loss: 0.17682376503944397\n",
      "step: 939, train_loss: 0.425383985042572\n",
      "step: 940, train_loss: 0.15323325991630554\n",
      "step: 941, train_loss: 0.27253812551498413\n",
      "step: 942, train_loss: 0.40051397681236267\n",
      "step: 943, train_loss: 0.26252132654190063\n",
      "step: 944, train_loss: 0.36027276515960693\n",
      "step: 945, train_loss: 0.24413682520389557\n",
      "step: 946, train_loss: 0.2559889256954193\n",
      "step: 947, train_loss: 0.5913635492324829\n",
      "step: 948, train_loss: 0.4429308772087097\n",
      "step: 949, train_loss: 0.30428066849708557\n",
      "step: 950, train_loss: 0.19419682025909424\n",
      "step: 951, train_loss: 0.36856937408447266\n",
      "step: 952, train_loss: 0.3363155722618103\n",
      "step: 953, train_loss: 0.11336052417755127\n",
      "step: 954, train_loss: 0.22904230654239655\n",
      "step: 955, train_loss: 0.3299371600151062\n",
      "step: 956, train_loss: 0.2657964825630188\n",
      "step: 957, train_loss: 0.26212382316589355\n",
      "step: 958, train_loss: 0.4720986485481262\n",
      "step: 959, train_loss: 0.4629327952861786\n",
      "step: 960, train_loss: 0.5255958437919617\n",
      "step: 961, train_loss: 0.35762909054756165\n",
      "step: 962, train_loss: 0.14348125457763672\n",
      "step: 963, train_loss: 0.28344595432281494\n",
      "step: 964, train_loss: 0.6470248699188232\n",
      "step: 965, train_loss: 0.41153180599212646\n",
      "step: 966, train_loss: 0.22974886000156403\n",
      "step: 967, train_loss: 0.46816009283065796\n",
      "step: 968, train_loss: 0.20601071417331696\n",
      "step: 969, train_loss: 0.38998448848724365\n",
      "step: 970, train_loss: 0.13217264413833618\n",
      "step: 971, train_loss: 0.263871431350708\n",
      "step: 972, train_loss: 0.3408353626728058\n",
      "step: 973, train_loss: 0.20207373797893524\n",
      "step: 974, train_loss: 0.17674696445465088\n",
      "step: 975, train_loss: 0.3470538854598999\n",
      "step: 976, train_loss: 0.4284045398235321\n",
      "step: 977, train_loss: 0.19784140586853027\n",
      "step: 978, train_loss: 0.3015782833099365\n",
      "step: 979, train_loss: 0.3857308030128479\n",
      "step: 980, train_loss: 0.12119059264659882\n",
      "step: 981, train_loss: 0.10218492150306702\n",
      "step: 982, train_loss: 0.13915622234344482\n",
      "step: 983, train_loss: 0.24476082623004913\n",
      "step: 984, train_loss: 0.5618008375167847\n",
      "step: 985, train_loss: 0.2633685767650604\n",
      "step: 986, train_loss: 0.4016868472099304\n",
      "step: 987, train_loss: 0.537518322467804\n",
      "step: 988, train_loss: 0.46022701263427734\n",
      "step: 989, train_loss: 0.26998811960220337\n",
      "step: 990, train_loss: 0.3080865740776062\n",
      "step: 991, train_loss: 0.24501310288906097\n",
      "step: 992, train_loss: 0.10096487402915955\n",
      "step: 993, train_loss: 0.3651563823223114\n",
      "step: 994, train_loss: 0.5030088424682617\n",
      "step: 995, train_loss: 0.21021872758865356\n",
      "step: 996, train_loss: 0.4040221571922302\n",
      "step: 997, train_loss: 0.17623412609100342\n",
      "step: 998, train_loss: 0.14268401265144348\n",
      "step: 999, train_loss: 0.2702880799770355\n",
      "step: 1000, train_loss: 0.1315762996673584\n",
      "step: 1001, train_loss: 0.5096171498298645\n",
      "step: 1002, train_loss: 0.27016928791999817\n",
      "step: 1003, train_loss: 0.20770546793937683\n",
      "step: 1004, train_loss: 0.2872815728187561\n",
      "step: 1005, train_loss: 0.190220445394516\n",
      "step: 1006, train_loss: 0.17308366298675537\n",
      "step: 1007, train_loss: 0.534778892993927\n",
      "step: 1008, train_loss: 0.26933857798576355\n",
      "step: 1009, train_loss: 0.19207458198070526\n",
      "step: 1010, train_loss: 0.45223742723464966\n",
      "step: 1011, train_loss: 0.21199144423007965\n",
      "step: 1012, train_loss: 0.15355128049850464\n",
      "step: 1013, train_loss: 0.26712819933891296\n",
      "step: 1014, train_loss: 0.24984286725521088\n",
      "step: 1015, train_loss: 0.252369225025177\n",
      "step: 1016, train_loss: 0.1635061353445053\n",
      "step: 1017, train_loss: 0.3814694285392761\n",
      "step: 1018, train_loss: 0.27648457884788513\n",
      "step: 1019, train_loss: 0.11893131583929062\n",
      "step: 1020, train_loss: 0.1781015694141388\n",
      "step: 1021, train_loss: 0.5003235340118408\n",
      "step: 1022, train_loss: 0.15444642305374146\n",
      "step: 1023, train_loss: 0.31850960850715637\n",
      "step: 1024, train_loss: 0.5516219139099121\n",
      "step: 1025, train_loss: 0.20956018567085266\n",
      "step: 1026, train_loss: 0.39969852566719055\n",
      "step: 1027, train_loss: 0.2107652723789215\n",
      "step: 1028, train_loss: 0.2465568482875824\n",
      "step: 1029, train_loss: 0.2831439971923828\n",
      "step: 1030, train_loss: 0.1761714667081833\n",
      "step: 1031, train_loss: 0.2256392538547516\n",
      "step: 1032, train_loss: 0.15832026302814484\n",
      "step: 1033, train_loss: 0.2988319993019104\n",
      "step: 1034, train_loss: 0.3473285138607025\n",
      "step: 1035, train_loss: 0.5514164566993713\n",
      "step: 1036, train_loss: 0.14526551961898804\n",
      "step: 1037, train_loss: 0.6191293597221375\n",
      "step: 1038, train_loss: 0.17872613668441772\n",
      "step: 1039, train_loss: 0.40989428758621216\n",
      "step: 1040, train_loss: 0.33052319288253784\n",
      "step: 1041, train_loss: 0.35218024253845215\n",
      "step: 1042, train_loss: 0.10492286086082458\n",
      "step: 1043, train_loss: 0.19614583253860474\n",
      "step: 1044, train_loss: 0.22317412495613098\n",
      "step: 1045, train_loss: 0.21263885498046875\n",
      "step: 1046, train_loss: 0.524053692817688\n",
      "step: 1047, train_loss: 0.2558181583881378\n",
      "step: 1048, train_loss: 0.4670153558254242\n",
      "step: 1049, train_loss: 0.37275126576423645\n",
      "step: 1050, train_loss: 0.2629609704017639\n",
      "step: 1051, train_loss: 0.22346967458724976\n",
      "step: 1052, train_loss: 0.1627080738544464\n",
      "step: 1053, train_loss: 0.1544293761253357\n",
      "step: 1054, train_loss: 0.2090354859828949\n",
      "step: 1055, train_loss: 0.24574419856071472\n",
      "step: 1056, train_loss: 0.5191115140914917\n",
      "step: 1057, train_loss: 0.1880262792110443\n",
      "step: 1058, train_loss: 0.49881622195243835\n",
      "step: 1059, train_loss: 0.2423485666513443\n",
      "step: 1060, train_loss: 0.1852513998746872\n",
      "step: 1061, train_loss: 0.21938827633857727\n",
      "step: 1062, train_loss: 0.22826404869556427\n",
      "step: 1063, train_loss: 0.29760441184043884\n",
      "step: 1064, train_loss: 0.2293391227722168\n",
      "step: 1065, train_loss: 0.6118284463882446\n",
      "step: 1066, train_loss: 0.22702930867671967\n",
      "step: 1067, train_loss: 0.15153314173221588\n",
      "step: 1068, train_loss: 0.503591775894165\n",
      "step: 1069, train_loss: 0.3425610661506653\n",
      "step: 1070, train_loss: 0.2858501076698303\n",
      "step: 1071, train_loss: 0.2907068133354187\n",
      "step: 1072, train_loss: 0.19511911273002625\n",
      "step: 1073, train_loss: 0.2761293351650238\n",
      "step: 1074, train_loss: 0.323526531457901\n",
      "step: 1075, train_loss: 0.2940306067466736\n",
      "step: 1076, train_loss: 0.16973580420017242\n",
      "step: 1077, train_loss: 0.3197549283504486\n",
      "step: 1078, train_loss: 0.6644763946533203\n",
      "step: 1079, train_loss: 0.33011531829833984\n",
      "step: 1080, train_loss: 0.22033338248729706\n",
      "step: 1081, train_loss: 0.2921907901763916\n",
      "step: 1082, train_loss: 0.17484188079833984\n",
      "step: 1083, train_loss: 0.44147181510925293\n",
      "step: 1084, train_loss: 0.17135635018348694\n",
      "step: 1085, train_loss: 0.2500917911529541\n",
      "step: 1086, train_loss: 0.6409441232681274\n",
      "step: 1087, train_loss: 0.3446236252784729\n",
      "step: 1088, train_loss: 0.23926082253456116\n",
      "step: 1089, train_loss: 0.2558751106262207\n",
      "step: 1090, train_loss: 0.15517598390579224\n",
      "step: 1091, train_loss: 0.4039004445075989\n",
      "step: 1092, train_loss: 0.28312546014785767\n",
      "step: 1093, train_loss: 0.2685285210609436\n",
      "step: 1094, train_loss: 0.32564377784729004\n",
      "step: 1095, train_loss: 0.20855236053466797\n",
      "step: 1096, train_loss: 0.2009725123643875\n",
      "step: 1097, train_loss: 0.47648173570632935\n",
      "step: 1098, train_loss: 0.3259199261665344\n",
      "step: 1099, train_loss: 0.2407999038696289\n",
      "step: 1100, train_loss: 0.17367368936538696\n",
      "step: 1101, train_loss: 0.11682012677192688\n",
      "step: 1102, train_loss: 0.4156543016433716\n",
      "step: 1103, train_loss: 0.38012832403182983\n",
      "step: 1104, train_loss: 0.25708475708961487\n",
      "step: 1105, train_loss: 0.3914521038532257\n",
      "step: 1106, train_loss: 0.2429884672164917\n",
      "step: 1107, train_loss: 0.23501437902450562\n",
      "step: 1108, train_loss: 0.1525505930185318\n",
      "step: 1109, train_loss: 0.3551146388053894\n",
      "step: 1110, train_loss: 0.5260040760040283\n",
      "step: 1111, train_loss: 0.4580439627170563\n",
      "step: 1112, train_loss: 0.22355683147907257\n",
      "step: 1113, train_loss: 0.13107942044734955\n",
      "step: 1114, train_loss: 0.26971012353897095\n",
      "step: 1115, train_loss: 0.2877403497695923\n",
      "step: 1116, train_loss: 0.13433659076690674\n",
      "step: 1117, train_loss: 0.20420555770397186\n",
      "step: 1118, train_loss: 0.16660946607589722\n",
      "step: 1119, train_loss: 0.34853002429008484\n",
      "step: 1120, train_loss: 0.16369421780109406\n",
      "step: 1121, train_loss: 0.27240613102912903\n",
      "step: 1122, train_loss: 0.23262852430343628\n",
      "step: 1123, train_loss: 0.3942078649997711\n",
      "step: 1124, train_loss: 0.4281731843948364\n",
      "step: 1125, train_loss: 0.35339462757110596\n",
      "step: 1126, train_loss: 0.2609889805316925\n",
      "step: 1127, train_loss: 0.4920332133769989\n",
      "step: 1128, train_loss: 0.24184159934520721\n",
      "step: 1129, train_loss: 0.1839519888162613\n",
      "step: 1130, train_loss: 0.24425344169139862\n",
      "step: 1131, train_loss: 0.4397077262401581\n",
      "step: 1132, train_loss: 0.1515386402606964\n",
      "step: 1133, train_loss: 0.28140145540237427\n",
      "step: 1134, train_loss: 0.2735038995742798\n",
      "step: 1135, train_loss: 0.48790204524993896\n",
      "step: 1136, train_loss: 0.3076210021972656\n",
      "step: 1137, train_loss: 0.18006935715675354\n",
      "step: 1138, train_loss: 0.2871129810810089\n",
      "step: 1139, train_loss: 0.14261481165885925\n",
      "step: 1140, train_loss: 0.2016095221042633\n",
      "step: 1141, train_loss: 0.5646576285362244\n",
      "step: 1142, train_loss: 0.4789496064186096\n",
      "step: 1143, train_loss: 0.18210828304290771\n",
      "step: 1144, train_loss: 0.16529326140880585\n",
      "step: 1145, train_loss: 0.18904170393943787\n",
      "step: 1146, train_loss: 0.3945170044898987\n",
      "step: 1147, train_loss: 0.603699803352356\n",
      "step: 1148, train_loss: 0.29707711935043335\n",
      "step: 1149, train_loss: 0.3618102967739105\n",
      "step: 1150, train_loss: 0.19303017854690552\n",
      "step: 1151, train_loss: 0.16818515956401825\n",
      "step: 1152, train_loss: 0.2495388388633728\n",
      "step: 1153, train_loss: 0.5242023468017578\n",
      "step: 1154, train_loss: 0.3231804370880127\n",
      "step: 1155, train_loss: 0.34271425008773804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1156, train_loss: 0.15346543490886688\n",
      "step: 1157, train_loss: 0.12605978548526764\n",
      "step: 1158, train_loss: 0.09769749641418457\n",
      "step: 1159, train_loss: 0.27980688214302063\n",
      "step: 1160, train_loss: 0.37660884857177734\n",
      "step: 1161, train_loss: 0.2699145972728729\n",
      "step: 1162, train_loss: 0.6002373099327087\n",
      "step: 1163, train_loss: 0.3234702944755554\n",
      "step: 1164, train_loss: 0.4998166263103485\n",
      "step: 1165, train_loss: 0.2244163155555725\n",
      "step: 1166, train_loss: 0.2039932906627655\n",
      "step: 1167, train_loss: 0.3474380373954773\n",
      "step: 1168, train_loss: 0.23250964283943176\n",
      "step: 1169, train_loss: 0.3335030972957611\n",
      "step: 1170, train_loss: 0.5885197520256042\n",
      "step: 1171, train_loss: 0.3650902509689331\n",
      "step: 1172, train_loss: 0.16106413304805756\n",
      "step: 1173, train_loss: 0.27086901664733887\n",
      "step: 1174, train_loss: 0.23789355158805847\n",
      "step: 1175, train_loss: 0.19393952190876007\n",
      "step: 1176, train_loss: 0.14840193092823029\n",
      "step: 1177, train_loss: 0.4217092990875244\n",
      "step: 1178, train_loss: 0.27005916833877563\n",
      "step: 1179, train_loss: 0.16303308308124542\n",
      "step: 1180, train_loss: 0.19402103126049042\n",
      "step: 1181, train_loss: 0.39447468519210815\n",
      "step: 1182, train_loss: 0.16398198902606964\n",
      "step: 1183, train_loss: 0.16958947479724884\n",
      "step: 1184, train_loss: 0.26083993911743164\n",
      "step: 1185, train_loss: 0.4366939067840576\n",
      "step: 1186, train_loss: 0.46158015727996826\n",
      "step: 1187, train_loss: 0.25084418058395386\n",
      "step: 1188, train_loss: 0.2492385357618332\n",
      "step: 1189, train_loss: 0.3377302289009094\n",
      "step: 1190, train_loss: 0.21016542613506317\n",
      "step: 1191, train_loss: 0.12945036590099335\n",
      "step: 1192, train_loss: 0.1334744393825531\n",
      "step: 1193, train_loss: 0.34799250960350037\n",
      "step: 1194, train_loss: 0.1767752766609192\n",
      "step: 1195, train_loss: 0.5415130853652954\n",
      "step: 1196, train_loss: 0.5139236450195312\n",
      "step: 1197, train_loss: 0.2519044876098633\n",
      "step: 1198, train_loss: 0.21496990323066711\n",
      "step: 1199, train_loss: 0.2142166793346405\n",
      "step: 1200, train_loss: 0.31607967615127563\n",
      "step: 1201, train_loss: 0.15403106808662415\n",
      "step: 1202, train_loss: 0.16838808357715607\n",
      "step: 1203, train_loss: 0.5710436105728149\n",
      "step: 1204, train_loss: 0.49656081199645996\n",
      "step: 1205, train_loss: 0.11668076366186142\n",
      "step: 1206, train_loss: 0.2976105213165283\n",
      "step: 1207, train_loss: 0.22627168893814087\n",
      "step: 1208, train_loss: 0.16666775941848755\n",
      "step: 1209, train_loss: 0.15019440650939941\n",
      "step: 1210, train_loss: 0.455325186252594\n",
      "step: 1211, train_loss: 0.2695932984352112\n",
      "step: 1212, train_loss: 0.18929821252822876\n",
      "step: 1213, train_loss: 0.34227001667022705\n",
      "step: 1214, train_loss: 0.16109803318977356\n",
      "step: 1215, train_loss: 0.46375247836112976\n",
      "step: 1216, train_loss: 0.20699390769004822\n",
      "step: 1217, train_loss: 0.27198562026023865\n",
      "step: 1218, train_loss: 0.15157200396060944\n",
      "step: 1219, train_loss: 0.27233511209487915\n",
      "step: 1220, train_loss: 0.38623037934303284\n",
      "step: 1221, train_loss: 0.21414437890052795\n",
      "step: 1222, train_loss: 0.20845606923103333\n",
      "step: 1223, train_loss: 0.19024977087974548\n",
      "step: 1224, train_loss: 0.2877868413925171\n",
      "step: 1225, train_loss: 0.6037440896034241\n",
      "step: 1226, train_loss: 0.41095617413520813\n",
      "step: 1227, train_loss: 0.17115363478660583\n",
      "step: 1228, train_loss: 0.2930438220500946\n",
      "step: 1229, train_loss: 0.21916577219963074\n",
      "step: 1230, train_loss: 0.5147980451583862\n",
      "step: 1231, train_loss: 0.2897067964076996\n",
      "step: 1232, train_loss: 0.30992674827575684\n",
      "step: 1233, train_loss: 0.2649848163127899\n",
      "step: 1234, train_loss: 0.18298543989658356\n",
      "step: 1235, train_loss: 0.30824947357177734\n",
      "step: 1236, train_loss: 0.40206900238990784\n",
      "step: 1237, train_loss: 0.4222913980484009\n",
      "step: 1238, train_loss: 0.2560829222202301\n",
      "step: 1239, train_loss: 0.17105038464069366\n",
      "step: 1240, train_loss: 0.3291187882423401\n",
      "step: 1241, train_loss: 0.31251269578933716\n",
      "step: 1242, train_loss: 0.4057072401046753\n",
      "step: 1243, train_loss: 0.5623691082000732\n",
      "step: 1244, train_loss: 0.3697240948677063\n",
      "step: 1245, train_loss: 0.21001288294792175\n",
      "step: 1246, train_loss: 0.16078495979309082\n",
      "step: 1247, train_loss: 0.5368460416793823\n",
      "step: 1248, train_loss: 0.15740589797496796\n",
      "step: 1249, train_loss: 0.27887454628944397\n",
      "step: 1250, train_loss: 0.33395615220069885\n",
      "step: 1251, train_loss: 0.20364218950271606\n",
      "step: 1252, train_loss: 0.368460476398468\n",
      "step: 1253, train_loss: 0.5514737367630005\n",
      "step: 1254, train_loss: 0.2478998303413391\n",
      "step: 1255, train_loss: 0.17380447685718536\n",
      "step: 1256, train_loss: 0.26212233304977417\n",
      "step: 1257, train_loss: 0.47396689653396606\n",
      "step: 1258, train_loss: 0.249271422624588\n",
      "step: 1259, train_loss: 0.3447043001651764\n",
      "step: 1260, train_loss: 0.11174465715885162\n",
      "step: 1261, train_loss: 0.1824823021888733\n",
      "step: 1262, train_loss: 0.26380839943885803\n",
      "step: 1263, train_loss: 0.3814055919647217\n",
      "step: 1264, train_loss: 0.198882594704628\n",
      "step: 1265, train_loss: 0.5457422733306885\n",
      "step: 1266, train_loss: 0.24775192141532898\n",
      "step: 1267, train_loss: 0.2575637996196747\n",
      "step: 1268, train_loss: 0.5060690641403198\n",
      "step: 1269, train_loss: 0.12608584761619568\n",
      "step: 1270, train_loss: 0.26296648383140564\n",
      "step: 1271, train_loss: 0.21340425312519073\n",
      "step: 1272, train_loss: 0.20539522171020508\n",
      "step: 1273, train_loss: 0.3181235194206238\n",
      "step: 1274, train_loss: 0.15516111254692078\n",
      "step: 1275, train_loss: 0.19976027309894562\n",
      "step: 1276, train_loss: 0.258098840713501\n",
      "step: 1277, train_loss: 0.2200234830379486\n",
      "step: 1278, train_loss: 0.10333068668842316\n",
      "step: 1279, train_loss: 0.22742588818073273\n",
      "step: 1280, train_loss: 0.13219031691551208\n",
      "step: 1281, train_loss: 0.2579007148742676\n",
      "step: 1282, train_loss: 0.3197774887084961\n",
      "step: 1283, train_loss: 0.41039201617240906\n",
      "step: 1284, train_loss: 0.4776812493801117\n",
      "step: 1285, train_loss: 0.42278003692626953\n",
      "step: 1286, train_loss: 0.6155036687850952\n",
      "step: 1287, train_loss: 0.45182859897613525\n",
      "step: 1288, train_loss: 0.25705111026763916\n",
      "step: 1289, train_loss: 0.17592039704322815\n",
      "step: 1290, train_loss: 0.4009736180305481\n",
      "step: 1291, train_loss: 0.3094272017478943\n",
      "step: 1292, train_loss: 0.32993724942207336\n",
      "step: 1293, train_loss: 0.4939858019351959\n",
      "step: 1294, train_loss: 0.3955574333667755\n",
      "step: 1295, train_loss: 0.19447526335716248\n",
      "step: 1296, train_loss: 0.2056267410516739\n",
      "step: 1297, train_loss: 0.1868555098772049\n",
      "step: 1298, train_loss: 0.2913471460342407\n",
      "step: 1299, train_loss: 0.289433091878891\n"
     ]
    }
   ],
   "source": [
    "# 小批次的大小\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "step = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 重覆100個時期\n",
    "    for epoch in range(100):\n",
    "        for x_batch, y_batch in get_batches(x_train, y_train, 32):\n",
    "            train_loss, _ = sess.run(\n",
    "                [loss, train_step],\n",
    "                feed_dict={\n",
    "                    x: x_batch,\n",
    "                    y: y_batch.reshape((-1, 1))\n",
    "                }\n",
    "            )\n",
    "            print('step: {}, train_loss: {}'.format(\n",
    "                step, train_loss\n",
    "            ))\n",
    "            step += 1\n",
    "    \n",
    "    pred_ = sess.run(\n",
    "        pred,\n",
    "        feed_dict={\n",
    "            x: x_test\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
